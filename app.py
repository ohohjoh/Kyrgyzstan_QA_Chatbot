import streamlit as st
import os
from dotenv import load_dotenv
# PyMuPDFLoaderëŠ” ì´ì œ PDF ì›ë³¸ ë¡œë“œì— ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘ì–´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.
# ì—¬ê¸°ì„œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°í–ˆìŠµë‹ˆë‹¤.
from langchain_community.document_loaders import UnstructuredMarkdownLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA

# .env íŒŒì¼ ë¡œë“œ (API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°)
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

# --- Streamlit UI ì„¤ì • (KBS ë””ìì¸ ì œê±°) ---
# st.markdownì„ í†µí•´ ì‚½ì…í–ˆë˜ ì»¤ìŠ¤í…€ CSS ìŠ¤íƒ€ì¼ ì •ì˜ ë¶€ë¶„ì„ ëª¨ë‘ ì œê±°í–ˆìŠµë‹ˆë‹¤.
# ë”°ë¼ì„œ Streamlitì˜ ê¸°ë³¸ UI í…Œë§ˆê°€ ì ìš©ë©ë‹ˆë‹¤.

st.set_page_config(page_title="í‚¤ë¥´ê¸°ìŠ¤ìŠ¤íƒ„ Q&A ì±—ë´‡", layout="wide") # í˜ì´ì§€ ì œëª© ìœ ì§€
st.title("ğŸ‡°ğŸ‡¬ í‚¤ë¥´ê¸°ìŠ¤ìŠ¤íƒ„ Q&A ì±—ë´‡") # ë©”ì¸ ì œëª© ìœ ì§€
st.markdown("PDF ë° ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•©ë‹ˆë‹¤. (ì¶œì²˜ í¬í•¨)") # ì„¤ëª… ë¬¸êµ¬ ìœ ì§€
st.markdown("---") # ì‹œê°ì  êµ¬ë¶„ì„ ì€ ìœ ì§€

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ìºì‹± ë° ëŒ€í™” ê¸°ë¡)
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "messages" not in st.session_state:
    st.session_state.messages = []
# ì„¸ì…˜ ìƒíƒœì— ì›ë³¸ ë¬¸ì„œ ë‚´ìš©ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ì¶”ê°€
if "full_documents_content" not in st.session_state:
    st.session_state.full_documents_content = {} # Key: source_key, Value: full_content

# --- ë¬¸ì„œ ë¡œë”© ë° ì²˜ë¦¬ í•¨ìˆ˜ (ìºì‹± ì ìš©) ---
@st.cache_resource
def load_and_process_documents(_folder_path="data/processed_md"): # <--- ê²½ë¡œë¥¼ ì „ì²˜ë¦¬ëœ MD í´ë”ë¡œ ë³€ê²½!
    all_documents = []
    current_full_documents_content = {}

    # PDF íŒŒì¼ ë¡œë“œ ë¶€ë¶„ì€ ì´ì œ ì œê±°í•˜ê³  Markdown íŒŒì¼ ë¡œë“œë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    # ì´ì „ ì£¼ì„ ì²˜ë¦¬ëœ PDF ë¡œë“œ ì½”ë“œëŠ” ì—¬ê¸°ì„œë„ ì œê±°í–ˆìŠµë‹ˆë‹¤.

    # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¡œë“œ
    st.info(f"ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¡œë“œ ì¤‘: {_folder_path}/*.md")
    md_loader = DirectoryLoader(
        _folder_path,
        glob="**/*.md",
        loader_cls=UnstructuredMarkdownLoader # UnstructuredMarkdownLoader ì‚¬ìš©
    )
    try:
        md_docs = md_loader.load()
        for doc in md_docs:
            source_path = doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ëŠ” MD ì¶œì²˜")
            base_name = os.path.basename(source_path)
            
            doc.metadata["source"] = base_name
            doc.metadata["type"] = "Markdown"
            doc.metadata["page"] = "N/A" # ë§ˆí¬ë‹¤ìš´ì€ í˜ì´ì§€ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ N/A

            # ì›ë³¸ ë¬¸ì„œ ë‚´ìš© ì €ì¥: íŒŒì¼ ì´ë¦„ë§Œìœ¼ë¡œ ê³ ìœ  í‚¤ ìƒì„±
            key = f"Markdown_{base_name}"
            current_full_documents_content[key] = doc.page_content
            
        all_documents.extend(md_docs)
        st.success(f"ë§ˆí¬ë‹¤ìš´ íŒŒì¼ {len(md_docs)}ê°œ ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        st.error(f"ë§ˆí¬down íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. '{_folder_path}' í´ë”ì— ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”. 'pip install \"unstructured[all-docs]\"' ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return None # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë°”ë¡œ ì¤‘ë‹¨

    if not all_documents:
        st.error("ë¡œë“œí•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. 'data/processed_md' í´ë”ì— ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.") # <--- ë©”ì‹œì§€ ë³€ê²½
        st.stop()
        return None

    # Step 2: ë¬¸ì„œ ì²­í¬ ë¶„í•  (RecursiveCharacterTextSplitter ì‚¬ìš©)
    st.info("ë¬¸ì„œ ì²­í¬ ë¶„í•  ì¤‘...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        # Markdown êµ¬ì¡°ë¥¼ ë” ì˜ í™œìš©í•˜ë„ë¡ separators ê°•í™”
        separators=["\n\n", "\n# ", "\n## ", "\n### ", "\n#### ", "\n##### ", "\n", ".", " ", ""], 
        length_function=len
    )
    split_docs = text_splitter.split_documents(all_documents)
    st.success(f"ì´ {len(split_docs)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ.")

    # Step 3: ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
    st.info("ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘... (OpenAI API í˜¸ì¶œ, ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    try:
        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        vectorstore = Chroma.from_documents(split_docs, embedding=embeddings)
        st.success("ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ!")
        
        # vectorstoreê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ë©´ ì›ë³¸ ë¬¸ì„œ ë‚´ìš©ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state.full_documents_content = current_full_documents_content
        return vectorstore
    except Exception as e:
        st.error(f"ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. OpenAI API í‚¤ê°€ ìœ íš¨í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
        return None

# --- ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ íë¦„ ---
if st.session_state.vectorstore is None:
    # ë¬¸ì„œ ë¡œë“œ ê²½ë¡œë¥¼ 'data/processed_md'ë¡œ ì§€ì •
    st.session_state.vectorstore = load_and_process_documents(_folder_path="data/processed_md")

# vectorstoreê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆì„ ê²½ìš°ì—ë§Œ ì±—ë´‡ ê¸°ëŠ¥ í™œì„±í™”
if st.session_state.vectorstore:
    @st.cache_resource
    def get_qa_chain(_vectorstore_instance):
        llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=0.1, model="gpt-3.5-turbo")
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=_vectorstore_instance.as_retriever(search_kwargs={"k": 5}), # kê°’ì€ í•„ìš”ì— ë”°ë¼ ì¡°ì ˆ
            return_source_documents=True
        )
        return qa_chain

    qa_chain = get_qa_chain(st.session_state.vectorstore)

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    prompt = st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.")

    if prompt:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                try:
                    result = qa_chain({"query": prompt})
                    answer = result["result"]
                    source_documents = result["source_documents"]

                    st.markdown(answer)

                    if source_documents:
                        st.markdown("---")
                        st.markdown("### ğŸ“š ë‹µë³€ ì°¸ê³  ìë£Œ")
                        unique_sources_for_display = {} 

                        for doc in source_documents:
                            source_file_name = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜')
                            source_type = doc.metadata.get('type', 'ë¬¸ì„œ ìœ í˜• ì—†ìŒ')
                            page_info = doc.metadata.get('page', 'N/A')
                            
                            source_key_for_content = ""
                            if source_type == 'PDF': # ì´ ë¶€ë¶„ì€ MDë§Œ ë¡œë“œí•˜ë¯€ë¡œ ì‚¬ì‹¤ìƒ í˜¸ì¶œë˜ì§€ ì•Šì§€ë§Œ, í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš°ë¥¼ ìœ„í•´ ìœ ì§€
                                source_key_for_content = f"PDF_{source_file_name}_page_{page_info}"
                            elif source_type == 'Markdown':
                                source_key_for_content = f"Markdown_{source_file_name}"
                            
                            unique_sources_for_display[source_key_for_content] = {
                                "display_name": f"**{source_type}:** `{source_file_name}`" + (f" (í˜ì´ì§€: {page_info})" if source_type == 'PDF' and page_info != 'N/A' else ""),
                                "content_key": source_key_for_content
                            }
                        
                        sorted_sources = sorted(unique_sources_for_display.values(), key=lambda x: x["display_name"])

                        if sorted_sources:
                            for source_info in sorted_sources:
                                with st.expander(source_info["display_name"]):
                                    content_key = source_info["content_key"]
                                    if content_key in st.session_state.full_documents_content:
                                        st.text_area(
                                            label="ê´€ë ¨ ë‚´ìš©", 
                                            value=st.session_state.full_documents_content[content_key],
                                            height=200, 
                                            disabled=True, 
                                            key=f"content_area_{content_key}" 
                                        )
                                    else:
                                        st.info("ì›ë³¸ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œ ë¡œë“œ ì‹œ ë¬¸ì œê°€ ìˆì—ˆê±°ë‚˜ í•´ë‹¹ ë¶€ë¶„ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        else:
                            st.info("ì°¸ê³ í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

                except Exception as e:
                    st.error(f"ì±—ë´‡ ì˜¤ë¥˜: {e}. API í‚¤ì™€ ëª¨ë¸ ì„¤ì •ì„ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                    answer = "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            
            st.session_state.messages.append({"role": "assistant", "content": answer})

# vectorstoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° (ë¡œë“œ ì‹¤íŒ¨ ì‹œ) ë©”ì‹œì§€ í‘œì‹œ
else:
    st.warning("ì±—ë´‡ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'data/processed_md' í´ë”ì— ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ì—†ê±°ë‚˜ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìƒë‹¨ì˜ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")