# app.py

# Streamlit Cloud í™˜ê²½ì—ì„œë§Œ pysqlite3ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì¡°ê±´ë¶€ ì„í¬íŠ¸
# ë¡œì»¬ ê°œë°œ í™˜ê²½ì—ì„œëŠ” ì˜¤ë¥˜ë¥¼ ë°©ì§€í•˜ê³ , ê¸°ë³¸ sqlite3ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
import os
if os.environ.get('STREAMLIT_SERVER_PORT') or os.environ.get('IS_STREAMLIT_CLOUD'):
    try:
        import pysqlite3
        import sys
        sys.modules["sqlite3"] = sys.modules["pysqlite3"]
    except ImportError:
        # Streamlit Cloud í™˜ê²½ì´ì§€ë§Œ pysqlite3ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°
        print("Warning: pysqlite3 not found, falling back to default sqlite3.")
        pass

import streamlit as st
from dotenv import load_dotenv

from langchain_community.document_loaders import UnstructuredMarkdownLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# OpenAI Embeddingsì™€ Chat ëª¨ë¸ì€ ê°ê° ì•„ë˜ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI

from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA

from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)
# from langchain.schema import SystemMessage # ì´ ì¤„ì€ ë” ì´ìƒ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì œê±°í–ˆìŠµë‹ˆë‹¤.

# .env íŒŒì¼ ë¡œë“œ (API í‚¤ë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°)
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

# --- Streamlit UI ì„¤ì • ---
st.set_page_config(page_title="í‚¤ë¥´ê¸°ìŠ¤ìŠ¤íƒ„ Q&A ì±—ë´‡", layout="wide")
st.title("ğŸ‡°ğŸ‡¬ í‚¤ë¥´ê¸°ìŠ¤ìŠ¤íƒ„ Q&A ì±—ë´‡")
st.markdown("ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•©ë‹ˆë‹¤. (ì¶œì²˜ í¬í•¨)")
st.markdown("---")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ìºì‹± ë° ëŒ€í™” ê¸°ë¡)
if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None
if "messages" not in st.session_state:
    st.session_state.messages = []
# ì„¸ì…˜ ìƒíƒœì— ì›ë³¸ ë¬¸ì„œ ë‚´ìš©ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ì¶”ê°€
if "full_documents_content" not in st.session_state:
    st.session_state.full_documents_content = {} # Key: source_key, Value: full_content

# --- ë¬¸ì„œ ë¡œë”© ë° ì²˜ë¦¬ í•¨ìˆ˜ (ìºì‹± ì ìš©) ---
@st.cache_resource(show_spinner=False) # ìŠ¤í”¼ë„ˆ ë©”ì‹œì§€ë¥¼ ì§ì ‘ ì œì–´í•˜ê¸° ìœ„í•´ show_spinner=False
def load_and_process_documents(_folder_path="data/processed_md"):
    all_documents = []
    current_full_documents_content = {}

    # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¡œë“œ
    st.info(f"âœ¨ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¡œë“œ ì¤‘: `{_folder_path}/*.md`")
    md_loader = DirectoryLoader(
        _folder_path,
        glob="**/*.md",
        loader_cls=UnstructuredMarkdownLoader
    )
    try:
        md_docs = md_loader.load()
        if not md_docs:
            st.error("âš ï¸ ë¡œë“œí•  ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. 'data/processed_md' í´ë”ì— íŒŒì¼ì„ ë„£ì–´ì£¼ì„¸ìš”.")
            st.stop() # ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ì•± ì‹¤í–‰ ì¤‘ë‹¨
            return None

        for doc in md_docs:
            source_path = doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ëŠ” MD ì¶œì²˜")
            base_name = os.path.basename(source_path)
            
            doc.metadata["source"] = base_name
            doc.metadata["type"] = "Markdown"
            doc.metadata["page"] = "N/A" # ë§ˆí¬ë‹¤ìš´ì€ í˜ì´ì§€ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ N/A

            key = f"Markdown_{base_name}"
            current_full_documents_content[key] = doc.page_content
            
        all_documents.extend(md_docs)
        st.success(f"âœ… ë§ˆí¬ë‹¤ìš´ íŒŒì¼ {len(md_docs)}ê°œ ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        st.error(f"âŒ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.error(f"'{_folder_path}' í´ë”ì— ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì´ ìˆëŠ”ì§€, ê·¸ë¦¬ê³  'pip install \"unstructured[all-docs]\"'ë¥¼ ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
        return None

    # Step 2: ë¬¸ì„œ ì²­í¬ ë¶„í•  (RecursiveCharacterTextSplitter ì‚¬ìš©)
    st.info("âœ‚ï¸ ë¬¸ì„œ ì²­í¬ ë¶„í•  ì¤‘...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n# ", "\n## ", "\n### ", "\n#### ", "\n##### ", "\n", ".", " ", ""], 
        length_function=len
    )
    split_docs = text_splitter.split_documents(all_documents)
    st.success(f"âœ… ì´ {len(split_docs)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ.")

    # Step 3: ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
    st.info("ğŸ§  ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘... (OpenAI API í˜¸ì¶œ, ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    try:
        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)
        vectorstore = Chroma.from_documents(split_docs, embedding=embeddings)
        st.success("âœ… ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ!")
        
        # vectorstoreê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ë©´ ì›ë³¸ ë¬¸ì„œ ë‚´ìš©ì„ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state.full_documents_content = current_full_documents_content
        
        return vectorstore
    except Exception as e:
        st.error(f"âŒ ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. OpenAI API í‚¤ê°€ ìœ íš¨í•œì§€, ì¶©ë¶„í•œ ì”ì•¡ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        st.stop()
        return None

# --- ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ íë¦„ ---
if st.session_state.vectorstore is None:
    st.session_state.vectorstore = load_and_process_documents(_folder_path="data/processed_md")

# vectorstoreê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆì„ ê²½ìš°ì—ë§Œ ì±—ë´‡ ê¸°ëŠ¥ í™œì„±í™”
if st.session_state.vectorstore:
    @st.cache_resource(show_spinner=False)
    def get_qa_chain(_vectorstore_instance):
        # 1) ì‹œìŠ¤í…œ ë©”ì‹œì§€ í…œí”Œë¦¿ ì •ì˜
        system_prompt = """
        ë‹¹ì‹ ì€ KBS(í•œêµ­ë°©ì†¡ê³µì‚¬)ì™€ ë°©ì†¡ê¸°ìˆ ì¸í˜‘íšŒë¡œ êµ¬ì„±ëœ ì»¨ì†Œì‹œì—„ì˜ ODA(ê³µì ê°œë°œì›ì¡°) ë° KOICA(í•œêµ­êµ­ì œí˜‘ë ¥ë‹¨) ì‚¬ì—… ì „ë¬¸ ìë¬¸ê°€ì…ë‹ˆë‹¤.
        ì»¨ì†Œì‹œì—„ì€ í˜„ì¬ í‚¤ë¥´ê¸°ìŠ¤ìŠ¤íƒ„ ì •ë¶€ê°€ ê³µëª¨í•˜ëŠ” ODA/KOICA ì‚¬ì—… ìˆ˜ì£¼ë¥¼ ëª©í‘œë¡œ í•˜ê³  ìˆìŠµë‹ˆë‹¤.
        ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œ(ë§ˆí¬ë‹¤ìš´ íŒŒì¼)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì»¨ì†Œì‹œì—„ì´ ì„±ê³µì ìœ¼ë¡œ ì‚¬ì—…ì„ ìˆ˜ì£¼í•˜ê³  ì§„í–‰í•  ìˆ˜ ìˆë„ë¡ ì „ëµì ì´ê³  ì‹¤ì§ˆì ì¸ ì¡°ì–¸ê³¼ ì •ë³´ë¥¼ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.

        ë‹¤ìŒ ì§€ì¹¨ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•˜ì—¬ ë‹µë³€í•´ì£¼ì„¸ìš”:
        1.  **ì£¼ìš” ì—­í• :** ODA ë° KOICA ì‚¬ì—… ì „ë¬¸ê°€ë¡œì„œ ì»¨ì†Œì‹œì—„(KBS ë° ë°©ì†¡ê¸°ìˆ ì¸í˜‘íšŒ)ì˜ ì…ì¥ì„ ëŒ€ë³€í•˜ë©°, ì‚¬ì—… ìˆ˜ì£¼ ë° ì‹¤í–‰ì— í•„ìš”í•œ ëª¨ë“  ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        2.  **ê´€ì  ìœ ì§€:** íŠ¹ë³„í•œ ìš”êµ¬ê°€ ì—†ëŠ” í•œ, í•­ìƒ KBSì™€ ë°©ì†¡ê¸°ìˆ ì¸í˜‘íšŒ ì»¨ì†Œì‹œì—„ì˜ ì´ìµê³¼ ê´€ì ì—ì„œ ë‹µë³€ì„ êµ¬ì„±í•©ë‹ˆë‹¤. ì‚¬ì—…ì˜ ê¸°íšŒ, ì»¨ì†Œì‹œì—„ì˜ ê°•ì , ì ì¬ì  ìœ„í—˜ ë° í•´ê²° ë°©ì•ˆ ë“±ì„ ì´ë“¤ì˜ ì…ì¥ì—ì„œ ë¶„ì„í•˜ê³  ì œì‹œí•©ë‹ˆë‹¤.
        3.  **ë‹µë³€ ì–¸ì–´:** ëª¨ë“  ë‹µë³€ì€ ë°˜ë“œì‹œ ëª…í™•í•˜ê³  ì „ë¬¸ì ì¸ **í•œêµ­ì–´**ë¡œ ì œê³µí•©ë‹ˆë‹¤.
        4.  **ì •ë³´ì˜ ê·¼ê±°:** ë‹µë³€ì˜ ëª¨ë“  ë‚´ìš©ì€ ì˜¤ì§ ë‹¹ì‹ ì—ê²Œ ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ í•©ë‹ˆë‹¤. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì ˆëŒ€ë¡œ ì¶”ì¸¡í•˜ê±°ë‚˜ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
        5.  **ì§ˆë¬¸ ë§¥ë½:** ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ODA/KOICA ì‚¬ì—… ê³µëª¨, í‚¤ë¥´ê¸°ìŠ¤ìŠ¤íƒ„ ì •ë¶€ì™€ì˜ í˜‘ë ¥, ì‚¬ì—… ì œì•ˆì„œ ì‘ì„±, ê¸°ìˆ  ì§€ì›, ì»¨ì†Œì‹œì—„ ìš´ì˜ ë“±ê³¼ ê´€ë ¨ëœ ê²ƒì„ì„ í•­ìƒ ì¸ì§€í•˜ê³  ë‹µë³€í•©ë‹ˆë‹¤.
        6.  **ë‹µë³€ í˜•ì‹:**
            * í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ê³ , í•„ìš”í•œ ê²½ìš° ìƒì„¸í•œ ì„¤ëª…ì„ ë§ë¶™ì…ë‹ˆë‹¤.
            * ì „ë¬¸ ìš©ì–´ëŠ” ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ë˜, ì»¨ì†Œì‹œì—„ ê´€ê³„ìë“¤ì´ ì´í•´í•˜ê¸° ì‰½ë„ë¡ í’€ì–´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤.
        7.  **ì¶œì²˜ ëª…ì‹œ:** ë‹µë³€ì˜ ë§ˆì§€ë§‰ì—ëŠ” í•­ìƒ "ë‹µë³€ ì°¸ê³  ìë£Œ" ì„¹ì…˜ì— ì œì‹œë˜ëŠ” ì¶œì²˜ ë¬¸ì„œì˜ ì œëª©ì„ ê°„ê²°í•˜ê²Œ ì–¸ê¸‰í•˜ì—¬ ë‹µë³€ì˜ ì‹ ë¢°ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.
        8.  **ì •ë³´ ë¶€ì¡± ì‹œ ëŒ€ì²˜:** ë§Œì•½ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ê°€ ì œê³µëœ ë¬¸ì„œ ë‚´ì— ì¶©ë¶„í•˜ì§€ ì•Šê±°ë‚˜ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´,
        "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë¬¸ì„œì—ì„œëŠ” í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ê´€ë ¨ ë¬¸ì„œë¥¼ ì¶”ê°€í•´ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        ë¼ê³  ë‹µë³€í•©ë‹ˆë‹¤.
        """

        # 2) ì‚¬ìš©ì ì§ˆë¬¸ ë¶€ë¶„ í…œí”Œë¦¿ ì •ì˜
        chat_prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_prompt),
            HumanMessagePromptTemplate.from_template("{query}")
        ])

        # 3) LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        llm = ChatOpenAI(
            openai_api_key=openai_api_key,
            temperature=0.1,
            model_name="gpt-3.5-turbo"
        )

        # 4) RetrievalQA ì²´ì¸ êµ¬ì„±
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=_vectorstore_instance.as_retriever(search_kwargs={"k": 5}),
            return_source_documents=True,
            chain_type_kwargs={"prompt": chat_prompt}
        )
        return qa_chain

    qa_chain = get_qa_chain(st.session_state.vectorstore)

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    prompt = st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.")

    if prompt:
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                try:
                    result = qa_chain({"query": prompt})
                    answer = result["result"]
                    source_documents = result["source_documents"]

                    st.markdown(answer)

                    if source_documents:
                        st.markdown("---")
                        st.markdown("### ğŸ“š ë‹µë³€ ì°¸ê³  ìë£Œ")
                        unique_sources_for_display = {} 

                        for doc in source_documents:
                            source_file_name = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜')
                            source_type = doc.metadata.get('type', 'ë¬¸ì„œ ìœ í˜• ì—†ìŒ')
                            page_info = doc.metadata.get('page', 'N/A')
                            
                            source_key_for_content = ""
                            if source_type == 'PDF': # í˜„ì¬ëŠ” MDë§Œ ë¡œë“œí•˜ë¯€ë¡œ ì‚¬ì‹¤ìƒ í˜¸ì¶œë˜ì§€ ì•Šì§€ë§Œ, ìœ ì§€
                                source_key_for_content = f"PDF_{source_file_name}_page_{page_info}"
                            elif source_type == 'Markdown':
                                source_key_for_content = f"Markdown_{source_file_name}"
                            
                            unique_sources_for_display[source_key_for_content] = {
                                "display_name": f"**{source_type}:** `{source_file_name}`" + (f" (í˜ì´ì§€: {page_info})" if source_type == 'PDF' and page_info != 'N/A' else ""),
                                "content_key": source_key_for_content
                            }
                        
                        sorted_sources = sorted(unique_sources_for_display.values(), key=lambda x: x["display_name"])

                        if sorted_sources:
                            for source_info in sorted_sources:
                                with st.expander(source_info["display_name"]):
                                    content_key = source_info["content_key"]
                                    if content_key in st.session_state.full_documents_content:
                                        st.text_area(
                                            label="ê´€ë ¨ ë‚´ìš©", 
                                            value=st.session_state.full_documents_content[content_key],
                                            height=200, 
                                            disabled=True, 
                                            key=f"content_area_{content_key}" 
                                        )
                                    else:
                                        st.info("âš ï¸ ì›ë³¸ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œ ë¡œë“œ ì‹œ ë¬¸ì œê°€ ìˆì—ˆê±°ë‚˜ í•´ë‹¹ ë¶€ë¶„ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        else:
                            st.info("ì°¸ê³ í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

                except Exception as e:
                    st.error(f"âŒ ì±—ë´‡ ì˜¤ë¥˜: {e}. API í‚¤ì™€ ëª¨ë¸ ì„¤ì •ì„ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
                    answer = "ì£„ì†¡í•©ë‹ˆë‹¤, ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
            
            st.session_state.messages.append({"role": "assistant", "content": answer})

# vectorstoreê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° (ë¡œë“œ ì‹¤íŒ¨ ì‹œ) ë©”ì‹œì§€ í‘œì‹œ
else:
    st.warning("âš ï¸ ì±—ë´‡ì„ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒë‹¨ì˜ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")